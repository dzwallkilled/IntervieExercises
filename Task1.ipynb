{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The project aims at developing a neural network to predict the Brownlow Votes in an AFL game.\n",
    "\n",
    "The content mainly consists of mainly three parts, Data, Model, and Training/Evaluation\n",
    "\n",
    "1. Data preparation\n",
    "\n",
    "2. Neural networks as predictive models\n",
    "\n",
    "3. Training and evaluation procedures\n",
    "\n",
    "\n",
    "# Data preparation\n",
    "This section mainly includes three parts\n",
    "\n",
    "1.1 Load raw data from RawData.csv file using the default csv library\n",
    "\n",
    "1.2 Outliers and noises removal\n",
    "\n",
    "1.3 Create Dataset and Dataloader objects in PyTorch framework\n",
    "\n",
    "\n",
    "## Load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#load data and get basic information\n",
    "with open('RawData.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    raw_data = []\n",
    "    for row in csv_reader:\n",
    "        raw_data.append(row)\n",
    "\n",
    "attribute_names = raw_data[0]\n",
    "attribute_names_to_idx = {n: i for i, n in enumerate(attribute_names)}\n",
    "raw_data = raw_data[1:]\n",
    "\n",
    "teams = []\n",
    "for row in raw_data:\n",
    "    if row[attribute_names_to_idx['Team']] not in teams:\n",
    "        teams.append(row[2])\n",
    "teams.sort()\n",
    "teams_to_idx = {t: i for i, t in enumerate(teams)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The raw data is organized into a dictionary for easy manipulation. There are totally $146607$ rows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def group_data_by_games(data):\n",
    "    games = {}\n",
    "    game_count = 0\n",
    "    prev_game = None\n",
    "    for row in data:\n",
    "        game = {'Date': row[attribute_names_to_idx['Date']],\n",
    "                'Season': row[attribute_names_to_idx['Season']],\n",
    "                'Round': row[attribute_names_to_idx['Round']],\n",
    "                'Home Team': row[attribute_names_to_idx['Home Team']],\n",
    "                'Away Team': row[attribute_names_to_idx['Away Team']],\n",
    "                'Home Score': row[attribute_names_to_idx['Home Score']],\n",
    "                'Away Score': row[attribute_names_to_idx['Away Score']],\n",
    "                'Margin': row[attribute_names_to_idx['Margin']]}\n",
    "        if prev_game is None or game != prev_game:\n",
    "            if game != prev_game:\n",
    "                game_count += 1\n",
    "            games[game_count] = game.copy()\n",
    "            games[game_count]['Stats'] = [row[10:-2] + [row[-1]]]\n",
    "            games[game_count]['Brownlow Votes'] = [row[attribute_names_to_idx['Brownlow Votes']]]\n",
    "            games[game_count]['Players'] = [row[attribute_names_to_idx['Name']]]\n",
    "            games[game_count]['Teams'] = [row[attribute_names_to_idx['Team']]]\n",
    "        else:\n",
    "            games[game_count]['Stats'].append(row[10:-2] + [row[-1]])\n",
    "            games[game_count]['Brownlow Votes'].append(row[attribute_names_to_idx['Brownlow Votes']])\n",
    "            games[game_count]['Players'].append(row[attribute_names_to_idx['Name']])\n",
    "            games[game_count]['Teams'].append(row[attribute_names_to_idx['Team']])\n",
    "        prev_game = game\n",
    "    return games\n",
    "\n",
    "\n",
    "data_by_games = group_data_by_games(raw_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Processing\n",
    "Data processing, includes noise removal, feature selection, and normalization."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    remove the noises and outliers, mainly including those matches of final series\n",
    "    remove the noisy attributes\n",
    "    :param data: a dict, output of data_by_games()\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_data = data.copy()\n",
    "    for i, d in list(new_data.items()):\n",
    "        rd = d['Round']\n",
    "        if len(d['Players']) != 44 or rd in ['GF', 'EF', 'QF', 'PF', 'SF']:\n",
    "            new_data.pop(i)\n",
    "    return new_data\n",
    "\n",
    "def transform_strings_to_numbers(data):\n",
    "    new_data = data.copy()\n",
    "    for i, d in new_data.items():\n",
    "        new_data[i]['Stats'] = np.array([list(map(float, dd)) for dd in d['Stats']])\n",
    "        new_data[i]['Brownlow Votes'] = np.array(list(map(float, d['Brownlow Votes'])), dtype=np.int)\n",
    "    return new_data\n",
    "\n",
    "cleaned_data = clean_data(data_by_games)\n",
    "transformed_data = transform_strings_to_numbers(cleaned_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In total, there are $3333$ games. After cleaning, there are totally $3177$ games. In the removed games, $3$ games have number of players fewer than 44, and other games are final serious."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_data_for_training_test(data: dict):\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    val_data = {}\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    for i, d in data.items():\n",
    "        if float(d['Season']) <= 2015:\n",
    "            train_data[train_count] = d\n",
    "            train_count += 1\n",
    "        else:\n",
    "            test_data[test_count] = d\n",
    "            test_count += 1\n",
    "\n",
    "    val_count = 0\n",
    "    for i in np.random.permutation(range(train_count))[:train_count // 5]:\n",
    "        val_data[val_count] = train_data[i]\n",
    "        val_count += 1\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = split_data_for_training_test(transformed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, create Dataset and Dataloader objects for training and test as given in PyTorch tutorials.\n",
    "\n",
    "### Normalization\n",
    "Considering that the Brownlow Votes are voted in each game, i.e. only the players in the same game will competent against each other, so data normalization should be done on each game as well."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def normalize_data(stats):\n",
    "    means = np.mean(stats, 0)\n",
    "    std = np.std(stats, 0)\n",
    "    return (stats-means)/std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Augmentation\n",
    "A simple augmentation method is shuffling the orders of players' stats. A good predictive model should give the correct answer regardless of the ordering.\n",
    "\n",
    "There are other augmentation methods, including random noises and random masks, in either stats or in targets. Due to time limitation, they are left to future work."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def data_augmentation(stats, votes):\n",
    "    # shuffle the order of players\n",
    "    idxes = range(stats.shape[0])\n",
    "    idxes = np.random.permutation(idxes)\n",
    "    return stats[idxes], votes[idxes]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementation\n",
    "\n",
    "The normalization and augmentation are implemented as functions that could be called for each data sample (each game) specifically.\n",
    "Dataset and Dataloader provides multi-processing property,\n",
    "so calling normalization and augmentation at this stage will be much more efficient."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Features and Labels\n",
    "\n",
    "### Features\n",
    "The players status are used as basic features.\n",
    "\n",
    "In addition, we could consider other information as features, such as team information, win/loss information, and even the player itself.\n",
    "Generally, I think a player in winner team has a higher chance to get the votes than a player in the other team.\n",
    "Also, a famous player could gain a higher chance when several players are competitive in a game.\n",
    "Hence, team information could be considered since players are usually associated with certain teams.\n",
    "\n",
    "### Labels\n",
    "In this task, the models learn to predict which player will win the most votes. Hence, it could be formulated as a classification problem.\n",
    "Given the stats of 44 players (in random order), classify the data into 1 of 44 classes.\n",
    "\n",
    "Meanwhile, to utilize the information of 2-voted and 1-voted players, the vote results of 44 players could be encoded into a distribution that has three peaks,\n",
    "with highest peak locating at 3-voted player, and second highest peak locating at 2-voted player,\n",
    "and the remaining peak locating at 1-voted player. In this case, the output of the model is a distribution, and could be trained with KL divergence loss.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_team_info(stats, teams):\n",
    "    tm = np.array([teams_to_idx[t] for t in teams])\n",
    "    return np.column_stack([stats, tm])\n",
    "\n",
    "\n",
    "def add_win_info(stats, home_team, teams, margin):\n",
    "    wins = np.array([float(margin) if home_team == t else -float(margin) for t in teams])\n",
    "    return np.column_stack([stats, wins])\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, data, augmentation=False, normalization=False, with_team_info=False, with_win_info=False):\n",
    "        super(Data, self).__init__()\n",
    "        self.data = data\n",
    "        self.ids_to_keys = {i: k for i, k in enumerate(data.keys())}\n",
    "        self.augmentation = augmentation\n",
    "        self.normalization = normalization\n",
    "        self.with_team_info = with_team_info\n",
    "        self.with_win_info = with_win_info\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[self.ids_to_keys[index]]\n",
    "        stats = data['Stats']\n",
    "        votes = data['Brownlow Votes']\n",
    "        if self.normalization:\n",
    "            stats = normalize_data(stats)\n",
    "        if self.with_team_info:\n",
    "            stats = add_team_info(stats, data['Teams'])\n",
    "        if self.with_win_info:\n",
    "            stats = add_win_info(stats, data['Home Team'], data['Teams'], data['Margin'])\n",
    "        if self.augmentation:\n",
    "            stats, votes = data_augmentation(stats, votes)\n",
    "        votes_idx = np.argmax(votes)\n",
    "        votes_dis = softmax(votes)\n",
    "\n",
    "        return stats, votes_idx, votes_dis\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "augmentation = True #hyper-param\n",
    "normalization = True #hyper-param\n",
    "with_team_info = True #hyper-param\n",
    "with_win_info = True #hyper-param\n",
    "batch_size = 128 #hyper-param\n",
    "train_loader = DataLoader(Data(train_data, augmentation=augmentation, normalization=normalization, with_team_info=with_team_info, with_win_info=with_win_info),\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=8,\n",
    "                          pin_memory=True)\n",
    "val_loader = DataLoader(Data(val_data, augmentation=augmentation, normalization=normalization, with_team_info=with_team_info, with_win_info=with_win_info),\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=8,\n",
    "                        pin_memory=True)\n",
    "test_loader = DataLoader(Data(test_data, augmentation=False, normalization=normalization, with_team_info=with_team_info, with_win_info=with_win_info),\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=8,\n",
    "                         pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model creation\n",
    "In this project, three types of deep neural networks are evaluated. Details are given in the following."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first model is multi-layer perceptron (MLP). All the players' status are concatenated into a 1-dim vector, and a MLP is used to cope the relationships within the vector."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=44*21, out_dim=44, p=0.4):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(input_dim, 1024),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm1d(1024),\n",
    "                                    nn.Linear(1024, 1024),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm1d(1024),\n",
    "                                    nn.Linear(1024,1024),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm1d(1024),\n",
    "                                    nn.Dropout(p),\n",
    "                                    nn.Linear(1024, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.layers(x)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second kind of networks are convolutional neural networks (CNNs).\n",
    "\n",
    "Considering applying the CNNs on images, a convolutional kernel is used to cope the relationships between neighbouring pixels, e.g. 3x3. Each pixel has several channels (e.g. 3 channels in RGB image, 1024 channels in feature maps).\n",
    "\n",
    "Similarly, in this model, each player is treated as a pixel, and their status are treated as channels. The convolutional kernel copes the relatinoship between neighbouring players in data, and its size determines the neighbour distances."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CONV(nn.Module):\n",
    "    def __init__(self, in_channels=21):\n",
    "        super(CONV, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Conv1d(in_channels, 64, 3, 1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm1d(64),\n",
    "                                    nn.Conv1d(64, 64, 3, 1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm1d(64),\n",
    "                                    nn.Conv1d(64, 128, 3, 2),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm1d(128),\n",
    "                                    nn.Conv1d(128, 256, 3, 2),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm1d(256))\n",
    "        self.fc = nn.Linear(256*11, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        out = self.fc(x.view(x.size(0), -1))\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The third kind of model is Transformer, which is proposed in the paper [Attention is all you need](https://arxiv.org/abs/1706.03762 \"Attention is all you need\").\n",
    "In this model, each player's stats will be treated as a vector, the relationships between players' status will be reasoned in the network. In theory, it is the most suitable model for this task."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ATT(nn.Module):\n",
    "    def __init__(self, input_dim, inner_dim, output_dim):\n",
    "        super(ATT, self).__init__()\n",
    "        self.K = nn.Conv1d(input_dim, inner_dim, 1, 1)\n",
    "        self.Q = nn.Conv1d(input_dim, inner_dim, 1, 1)\n",
    "        self.V = nn.Conv1d(input_dim, inner_dim, 1, 1)\n",
    "        self.out = nn.Conv1d(inner_dim, output_dim, 1, 1)\n",
    "        self.identiy = lambda x: x if output_dim == input_dim else nn.Conv1d(input_dim, output_dim, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        s = torch.matmul(q, k.transpose(1, 2))\n",
    "        s = F.softmax(s, 1)\n",
    "        output = self.identiy(x) + self.out(torch.matmul(v, s))\n",
    "        return output\n",
    "\n",
    "\n",
    "class TRANSFORMER(nn.Module):\n",
    "    def __init__(self, input_dim=21, ):\n",
    "        super(TRANSFORMER, self).__init__()\n",
    "        self.layers = nn.Sequential(ATT(input_dim, 32, 64),\n",
    "                                    ATT(64, 64, 128),\n",
    "                                    ATT(128, 64, 128),\n",
    "                                    ATT(128, 64, 128))\n",
    "        self.fc = nn.Linear(44*128, 44)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.layers(x).view(x.size(0), -1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_network(network_choice, with_win_info, with_team_info):\n",
    "    feature_dim = 21\n",
    "    if with_win_info:\n",
    "        feature_dim += 1\n",
    "    if with_team_info:\n",
    "        feature_dim += 1\n",
    "    if network_choice == 'mlp':\n",
    "        return MLP(44*feature_dim)\n",
    "    elif network_choice == 'conv':\n",
    "        return CONV(feature_dim)\n",
    "    elif network_choice == 'transformer':\n",
    "        return TRANSFORMER(feature_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss function and Accuracy measurement\n",
    "Loss consists of two parts, the cross entropy loss for classification, and the KL divergence loss for distribution.\n",
    "\n",
    "Top-k accuracy is used as metric, i.e. select the $k$ indexes that have the highest values, if one of them is the target, the prediction is considered as correct.\n",
    "Top-1 is conventional accuracy, and top-3 and top-5 accuracies are also used here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_function(output, target, target_dis=None):\n",
    "    T = 5 #hyper-parameter\n",
    "    alpha = 0.5 #hyper-parameter\n",
    "    if target_dis is None:\n",
    "        return nn.CrossEntropyLoss()(output, target)\n",
    "    else:\n",
    "        return nn.KLDivLoss()(F.log_softmax(output/T, dim=1),\n",
    "                             F.softmax(target_dis/T, dim=1)) * (alpha * T * T) + \\\n",
    "              F.cross_entropy(output, target) * (1. - alpha)\n",
    "\n",
    "\n",
    "def metric_function(output, target, topk=1):\n",
    "    topks, topk_idxes = torch.topk(output, topk, dim=1)\n",
    "    count = 0\n",
    "    for p, t in zip(topk_idxes, target):\n",
    "        if t in p:\n",
    "            count += 1\n",
    "    acc = count * 1.0 / len(target)\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training and evaluation framework\n",
    "The nework is trained on training data and evaluated on validation data iteratively for $30$ epochs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each epoch, run the following functions iteratively."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, dataloader, disstill=False):\n",
    "    model.train()\n",
    "\n",
    "    for idx, (data, target, target_dis) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        data, target, target_dis = data.float().to(device), target.to(device), target_dis.float().to(device)\n",
    "        output = model(data)\n",
    "        if disstill:\n",
    "            loss = loss_function(output, target, target_dis)\n",
    "        else:\n",
    "            loss = loss_function(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % 20 == 0:\n",
    "            print(f'Epoch {epoch} batch {idx}: loss {loss.item()}')\n",
    "    return\n",
    "\n",
    "\n",
    "def evaluate(epoch, model, dataloader, is_test=False):\n",
    "    model.eval()\n",
    "    top1 = 0\n",
    "    top3 = 0\n",
    "    top5 = 0\n",
    "    for idx, (data, target, _) in enumerate(dataloader):\n",
    "        data, target = data.float().to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        top1 += metric_function(output, target, topk=1) * len(target)\n",
    "        top3 += metric_function(output, target, topk=3) * len(target)\n",
    "        top5 += metric_function(output, target, topk=5) * len(target)\n",
    "        # if idx % 10 == 0:\n",
    "        #     print(f'Eval {epoch} batch {idx}: Acc {accuracy}')\n",
    "\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    print(f'Test/Eval epoch {epoch} top1 is {top1/num_samples}, top3: {top3/num_samples}, top5: {top5/num_samples}')\n",
    "    return top1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The framework"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "network_type = 'transformer'  # hyper-parameter\n",
    "lr = 0.01 #hyper-param\n",
    "momentum = 0.9 #hyper-param\n",
    "wd = 1e-4 #hyper-param\n",
    "lr_steps = [10, 20, 25] #hyper-param\n",
    "epochs = 30 #hyper-param\n",
    "network = build_network(network_type, with_win_info, with_team_info)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "network = network.to(device)\n",
    "optimizer = optim.SGD(network.parameters(), lr=lr, momentum=momentum, weight_decay=wd, nesterov=True)\n",
    "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, lr_steps, gamma=0.1)\n",
    "\n",
    "best_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    lr_scheduler.step(epoch)\n",
    "    train(epoch, network, optimizer, train_loader)\n",
    "    mean_acc = evaluate(epoch, network, val_loader)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(network.state_dict(), f'epoch{epoch}.pth')\n",
    "    if best_acc < mean_acc:\n",
    "        best_acc = mean_acc\n",
    "        torch.save(network.state_dict(), 'best.pth')\n",
    "\n",
    "network.load_state_dict(torch.load('best.pth'))\n",
    "evaluate(-1, network, test_loader, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results and Discussions\n",
    "We can explore different models (MLP/ConvNet/AttentionModel) and hyper-parameters(Optimizer/Learning rate/Number of Layers/Network Depth and width/etc.) to seek the best model with the highest top1 (or top3) accuracy.\n",
    "\n",
    "The best results (there are better settings for sure) are (top1/top3/top5 accuracies in %) __53.5/81.4/90.5__\n",
    "\n",
    "Some findings\n",
    "\n",
    "1. Transformer generally achieves the best performance, with top1/top3/top5 accuracies equalling to 53.5/81.4/90.5. Conv achieves similar performances, while MLP achieves worst performances, with a drop of more than 10 percentage on top1 accuracy.\n",
    "2. Batch size in training does not affect the results much, as long as it is smaller than 256. In experiments, batch sizes of 32/64/128 achieve similar performances.\n",
    "3. Data normalization improves the performances a bit, with 4 percentage improvement.\n",
    "4. Data augmentation improves the performances a lot, with more than 10 percentage improvement.\n",
    "5. The win/loss information increases the accuracy by  more than 5 percentage, and the team information has little impact on the results.\n",
    "\n",
    "6. The 2-voted and 1-voted players information does not help the prediction. On the contrary, it reduces the accuracies. The smaller T (temperature), the more accuracy drops.\n",
    "\n",
    "## Future work\n",
    "1. Explore architectures of Transformer network\n",
    "2. Explore player information in prediction\n",
    "3. Explore more hyper-parameters, such as more training epochs\n",
    "4. Explore more data augmentation methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}